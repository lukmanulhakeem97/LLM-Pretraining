# Pretraining of GPT2
This is a llm pretraining of GPT2 architecture on a small dataset from scratch. This a re-implementation of pretraining done in [LLM-from-sctach](https://github.com/rasbt/LLMs-from-scratch).

